# Split between independent and dependent variables
X_train <-  csv_train[,1:3]
Y_train <-  csv_train[,4]
Y_train
dim(Y_train)
# Run xgb.cv
cv <- xgb.cv(data = as.matrix(X_train), #Use as.matrix() to convert the data frame to a matrix.
label = Y_train, #csv$LowCloud,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0   # silent
)
# Get the evaluation log
elog <- cv$evaluation_log
head(elog)
# Determine and print how many trees minimize training and test error
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean) which will be the lowest RMSE of the tree counts
ntrees.test  = which.min(test_rmse_mean))    # find the index of min(test_rmse_mean)
# Assign it to use for the next model
ntrees <- elog %>%
summarize(ntrees.train = which.min(train_rmse_mean))
ntrees
X_test <-  csv_test[,1:3]
Y_test<-  csv_test[,4]
# Make predictions from the xgb model for the test group
csv$prediction <- predict(lcc_xgb, as.matrix(X_test))
# Run xgboost
lcc_xgb <- xgboost(data = as.matrix(X), # training data as matrix
label = Y,  # column of outcomes
nrounds = 23, # number of trees to build (ntrees output)
objective = "reg:linear", # objective
eta = 0.3,
depth = 6,
verbose = 0  # silent
)
# Run xgb.cv
cv <- xgb.cv(data = as.matrix(X_train), #Use as.matrix() to convert the data frame to a matrix.
label = Y_train, #csv$LowCloud,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0   # silent
)
# Get the evaluation log
elog <- cv$evaluation_log
head(elog)
# Determine and print how many trees minimize training and test error
ntrees <- elog %>%
summarize(ntrees.train = which.min(train_rmse_mean))
ntrees
# Run xgboost
lcc_xgb <- xgboost(data = as.matrix(X_train), # training data as matrix
label = Y_train,  # column of outcomes
nrounds = 21, # number of trees to build (ntrees output)
objective = "reg:linear", # objective
eta = 0.3,
depth = 6,
verbose = 0  # silent
)
# Create a vector of N uniform random variables
Set.seed(1)
# Create a vector of N uniform random variables
set.seed(1)
gp <- runif(N)
# Use gp to create the training set (80% of data) and test (20% of data)
csv_train <- csv[gp < 0.8, ]
csv_test <- csv[gp >= 0.8, ]
# Split between independent and dependent variables
X_train <-  csv_train[,1:3]
Y_train <-  csv_train[,4]
X_test <-  csv_test[,1:3]
Y_test<-  csv_test[,4]
# Run xgb.cv
cv <- xgb.cv(data = as.matrix(X_train), #Use as.matrix() to convert the data frame to a matrix.
label = Y_train, #csv$LowCloud,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0   # silent
)
# Get the evaluation log
elog <- cv$evaluation_log
head(elog)
# Determine and print how many trees minimize training and test error
ntrees <- elog %>%
summarize(ntrees.train = which.min(train_rmse_mean))
ntrees
# Run xgboost
lcc_xgb <- xgboost(data = as.matrix(X_train), # training data as matrix
label = Y_train,  # column of outcomes
nrounds = 22, # number of trees to build (ntrees output)
objective = "reg:linear", # objective
eta = 0.3,
depth = 6,
verbose = 0  # silent
)
# Make predictions from the xgb model for the test group
csv$prediction <- predict(lcc_xgb, as.matrix(X_test))
csv_test$prediction <- predict(lcc_xgb, as.matrix(X_test))
csv_test
# Plot predictions (on x axis) vs actual
ggplot(csv_test, aes(x = prediction, y = LowCloud)) +
geom_point() +
geom_abline()
# Calculate Mean absolute error
csv %>%
mutate(residuals = LowCloud - prediction) %>%
summarize(MAE = (mean(residuals)))
# Calculate Mean absolute error
csv_test %>%
mutate(residuals = LowCloud - prediction) %>%
summarize(MAE = (mean(residuals)))
# Calculate Root mean square error
csv_test %>%
mutate(residuals = LowCloud - prediction) %>%
summarize(RMSE = sqrt(mean(residuals^2)))
# Run xgb.cv
cv <- xgb.cv(data = as.matrix(X_train), #Use as.matrix() to convert the data frame to a matrix.
label = Y_train, #csv$LowCloud,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 15,
seed = 1,
early_stopping_rounds = 10,
verbose = 0   # silent
)
# Get the evaluation log
elog <- cv$evaluation_log
head(elog)
# Determine and print how many trees minimize training and test error
ntrees <- elog %>%
summarize(ntrees.train = which.min(train_rmse_mean))
ntrees
model <- xgb.dump(xgb, with.stats = T)
model[1:10] #This statement prints top 10 nodes of the model
model <- xgb.dump(lcc_xgb, with.stats = T)
model[1:10] #This statement prints top 10 nodes of the model
# Get the feature real names
names <- dimnames(data.matrix(X[,-1]))[[2]]
# Compute feature importance matrix
importance_matrix <- xgb.importance(names, model = lcc_xgb)
# Nice graph
xgb.plot.importance(importance_matrix[1:10,])
# Compute feature importance matrix
importance_matrix <- lcc_xgb.importance(names, model = lcc_xgb)
importance_matrix <- xgb.importance(model = lcc_xgb)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
xgb.plot.tree(model = lcc_xgb)
install.packages("DiagrammeR")
xgb.plot.tree(model = lcc_xgb)
# Calculates the Gradient Boosting Model for low cloud cover
if (!("xgboost" %in% installed.packages())) {
install.packages("xgboost")
}
library(xgboost)
library(dplyr)
library(ggplot2)
csv <- read.csv("/Users/claregaffey/Documents/RClass/test_xgbdata.csv")
class(csv)
head(csv)
# First, split dataset into testing and training
# Use nrow to get the number of rows in dataframe
(N <- nrow(csv))
# Calculate how many rows 80% of N should be
(target <- round(N * 0.8))
# Create a vector of N uniform random variables
set.seed(1)
gp <- runif(N)
# Use gp to create the training set (80% of data) and test (20% of data)
csv_train <- csv[gp < 0.8, ]
csv_test <- csv[gp >= 0.8, ]
# Split between independent and dependent variables
X_train <-  csv_train[,1:3]
Y_train <-  csv_train[,4]
X_test <-  csv_test[,1:3]
Y_test<-  csv_test[,4]
# Run xgb.cv
cv <- xgb.cv(data = as.matrix(X_train), #Use as.matrix() to convert the data frame to a matrix.
label = Y_train, #csv$LowCloud,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 15,
seed = 1,
early_stopping_rounds = 10,
verbose = 0   # silent
)
# Get the evaluation log
elog <- cv$evaluation_log
head(elog)
# Determine and print how many trees minimize training and test error
ntrees <- elog %>%
summarize(ntrees.train = which.min(train_rmse_mean))
ntrees
# Run xgboost
lcc_xgb <- xgboost(data = as.matrix(X_train), # training data as matrix
label = Y_train,  # column of outcomes
nrounds = 21, # number of trees to build (ntrees output)
objective = "reg:linear", # objective
eta = 0.3,
seed = 1,
depth = 15,
verbose = 0  # silent
)
# Make predictions from the xgb model for the test group
csv_test$prediction <- predict(lcc_xgb, as.matrix(X_test))
# Can also make predictions for the entire dataset
# csv$prediction <- predict(lcc_xgb, as.matrix(X))
# Plot predictions (x axis) vs actual (y axis)
ggplot(csv_test, aes(x = prediction, y = LowCloud)) +
geom_point() +
geom_abline()
## Accuracy assessment
# Calculate Mean absolute error
csv_test %>%
mutate(residuals = LowCloud - prediction) %>%
summarize(MAE = (mean(residuals)))
# Calculate Root mean square error
csv_test %>%
mutate(residuals = LowCloud - prediction) %>%
summarize(RMSE = sqrt(mean(residuals^2)))
# look at the variable importance from the model
importance_matrix <- xgb.importance(model = lcc_xgb)
importance_matrix
xgb.plot.importance(importance_matrix = importance_matrix)
library(sp)
library(tidyverse)
library(sf)
library(ncdf4)
library(raster)
library(rgdal)
library(ggspatial) #Requires ggplot2
# 1.
# DBO3 Shapefile for ROI
dbo3 <- "/Users/claregaffey/OneDrive - Clark University/R_Project/DBO3_shapefile/Dbo3.shp" %>%
st_read()
# 2.
##############################
# Sea Ice Concentration example
si <- "/Volumes/My Passport/RProject2021/SeaIce_MonthlySB2/SB2_1978_12_month.rst"
# create raster brick
var.sictif <- brick(si)#varname="chlor_a")
#project
crs(var.sictif) <- "EPSG:3413"
#crop
sicrop <- crop(x = var.sictif, y = dbo3)
# Visualize the entire file and the ROI
ggplot() +
layer_spatial(var.sictif, aes(fill = stat(band1))) +
scale_fill_continuous(na.value = NA) + layer_spatial(dbo3)
# Visualize the entire file and the ROI
ggplot() +
layer_spatial(var.sictif, aes(fill = stat(band1))) +
scale_fill_continuous(na.value = NA) + layer_spatial(dbo3) +
ggptitle("Sea Ice Concentration") + labs(fill = "SIC (%)")
# Visualize the entire file and the ROI
ggplot() +
layer_spatial(var.sictif, aes(fill = stat(band1))) +
scale_fill_continuous(na.value = NA) + layer_spatial(dbo3) +
ggtitle("Sea Ice Concentration") + labs(fill = "SIC (%)")
# The cropped sea ice concentration to DBO3
ggplot() +
layer_spatial(sicrop)
# The cropped sea ice concentration to DBO3
ggplot() +
layer_spatial(sicrop) +
ggtitle("Sea Ice Concentration of DBO3 (Chukchi Sea)") + labs(fill = "SIC (%)")
# 4.
#####################
# Cloud data example
lcl <- "/Volumes/My Passport/RProject2021/lcdc.mon.mean.nc" #Users/claregaffey/Downloads/lcdc.mon.mean.nc"#/
lcdc <- nc_open(lcl)
#check out the netcdf contents
lcdc
# create raster brick
var.nc1<-brick(lcl,varname="lcdc")
# Check out the contents
var.nc1
# reproject raster data to match the other raster datasets
# (will only work if i plug in the index)
lay1_clo <- projectRaster(var.nc1[[1]], crs = crs(var.sictif))
# check that it reprojected
crs(lay1_clo)
# crop to DBO3 region of interest (ROI)
clodbo3varlay1 <- crop(x = lay1_clo, y = dbo3)
# The reprojected low cloud cover and our region of interest
ggplot() +
layer_spatial(lay1_clo, aes(fill = stat(band1))) +
scale_fill_continuous(na.value = NA) +
layer_spatial(dbo3)
# The cropped DBO3 ROI of our cloud layer
ggplot() +
layer_spatial(clodbo3varlay1)
# The cropped DBO3 ROI of our cloud layer
ggplot() +
layer_spatial(clodbo3varlay1) +
ggtitle("Low Cloud Concentration of DBO3 (Chukchi Sea)") +
labs(fill = "CC (%)")
library(sp)
library(tidyverse)
library(sf)
library(ncdf4)
library(raster)
library(rgdal)
library(ggspatial) #
#^^^^^^^^^^^^^^^^^^^^^^^
# Bring in all of the sea ice time series data
mystack <- stack()
files <- list.files(path="/Volumes/My Passport/RProject2021/SeaIce_MonthlySB2/",
pattern="*.rst", full.names=TRUE, recursive=FALSE)
files
files <- list.files(path="/Volumes/My Passport/RProject2021/SeaIce_MonthlySB2",
pattern="*.rst", full.names=TRUE, recursive=FALSE)
files <- list.files(path="/Volumes/My Passport/RProject2021/SeaIce_MonthlySB2",
pattern="*.rst", full.names=TRUE, recursive=FALSE)
files
files <- list.files(path="/Volumes/My Passport/RProject2021/MODIS_chl/",
pattern="*.nc", full.names=TRUE, recursive=FALSE)
files
files <- list.files(path="/Volumes/My Passport/RProject2021/MODIS_chl",
pattern="*.nc", full.names=TRUE, recursive=FALSE)
files
substr(files,5,8)
substr(files,35,48)
substr(files,48,68)
substr(files,48,60)
substr(files,53,59)
substr(files,53,56)
pat <- seq(1:12)
pat
paste0(substr(files,53,56), "_", pat)
paste0(substr(files[4],53,56), "_", pat)
paste0(substr(files[-4],53,56), "_", pat)
class(files)
library(rlist)
paste0(substr(list.skip(files, 4),53,56), "_", pat)
install.packages("rlist")
library(rlist)
paste0(substr(list.skip(files, 4),53,56), "_", pat)
paste0(substr(list.skip(files, 3),53,56), "_", pat)
paste0(substr(list.skip(files, 4),53,56), "_", pat)
substr(files,53,59)
paste0(substr(list.skip(files, 4),53,56), "_", pat)
pat <- seq(1:11)
paste0(substr(list.skip(files, 4),53,56), "_", pat)
paste0(files,53,56), "_", (files,57,59), "_", pat)
paste0(files(53,56), "_", (files(57,59), "_", pat)
# Run for all sea ice data
NARR_dataprep(var.sic, dbo3)
# Run for all chlorophyll data
NARR_dataprep(var.chla, dbo3)
# Append the dates
NARR_dataprep <- function(NARR_brick, ROI) {
ek <- dim(NARR_brick)
time <- list()
meanlcl <- list()
counter <- 0
for (i in 1:ek[3]){
r <- subset(NARR_brick,i) %>% # running each time slice (works best)
projectRaster(crs = crs(var.sictif))  %>% #will not work with EPSG#
crop(y = ROI) #%>% # crop to the DBO3 extent
#resample(y = chldbo3varlay1) # Resamples to the chlorophyll pixel extents
counter <-  counter + 1 # keep track of which layer we are on in the console
print(paste0(counter, " out of ", ek[3]))
time <- append(time, names(r)) # add raster name to a list
k <- cellStats(x = r, stat = "mean") # calculate a mean over ROI
meanlcl <- append(meanlcl, k) # add averaged variable to a list
}
# make a dataframe with the raster name (time) and averaged variable lists
nam <- paste0(deparse(substitute(NARR_brick)), ".csv") # for file naming
df <- do.call(rbind, Map(data.frame, Time=time, Variable=meanlcl))
names(df)[names(df) == 'Variable'] <- substr(nam,5,8) #rename var column
df$Year.month.day <-  substr(df$Time,2,11) # new column for date info
df$Year_julianday_month <- paste0(files,53,56), "_", (files,57,59), "_", pat)
# export to a csv
write.csv(df, file = paste("/Users/claregaffey/Documents/RClass/", nam),
row.names = FALSE)#here::here(paste("external/data/", nam)))
return(head(df)) # display some rows of our dataframe
}
paste0(files(53,56), "_", (files(57,59), "_", pat)
#for checking the dates during the data merge
# (skipping the first incomplete year)
paste0(substr(files, 53, 56), "_", substr(files, 57, 59), "_", pat)
# Modified function to accomodate the chlorophyll file dates
CHL_dataprep <- function(NARR_brick, ROI) {
ek <- dim(NARR_brick)
time <- list()
meanlcl <- list()
counter <- 0
for (i in 1:ek[3]){
r <- subset(NARR_brick,i) %>% # running each time slice (works best)
projectRaster(crs = crs(var.sictif))  %>% #will not work with EPSG#
crop(y = ROI) #%>% # crop to the DBO3 extent
#resample(y = chldbo3varlay1) # Resamples to the chlorophyll pixel extents
counter <-  counter + 1 # keep track of which layer we are on in the console
print(paste0(counter, " out of ", ek[3]))
time <- append(time, names(r)) # add raster name to a list
k <- cellStats(x = r, stat = "mean") # calculate a mean over ROI
meanlcl <- append(meanlcl, k) # add averaged variable to a list
}
# make a dataframe with the raster name (time) and averaged variable lists
nam <- paste0(deparse(substitute(NARR_brick)), ".csv") # for file naming
df <- do.call(rbind, Map(data.frame, Time=time, Variable=meanlcl))
names(df)[names(df) == 'Variable'] <- substr(nam,5,8) #rename var column
df$Year.month.day <- paste0(substr(files, 53, 56), "_",
substr(files, 57, 59), "_", pat)
# export to a csv
write.csv(df, file = paste("/Users/claregaffey/Documents/RClass/", nam),
row.names = FALSE)#here::here(paste("external/data/", nam)))
return(head(df)) # display some rows of our dataframe
}
# Run for all chlorophyll data
CHL_dataprep(var.chla, dbo3)
# 3.
#####################
#Chlorophyll
#^^^^^^^^^^^^^^^^^^^^^^^
# This code loads in chl netcdf, converts to a raster object, reprojects it to
# match the other files(SIC, DBO3 bounding box ROI), crops it to the DBO3 ROI,
# and visualizes the data.
## Clean version of chla:
chl <- "/Volumes/My Passport/RProject2021/MODIS_chl/A20031822003212.L3m_MO_CHL.x_chlor_a.nc"
chla <- nc_open(chl)
#check out the netcdf contents
chla
# create raster brick
var.chl<-brick(chl,varname="chlor_a")
# Modified function to accomodate the chlorophyll file dates
CHL_dataprep <- function(NARR_brick, ROI) {
ek <- dim(NARR_brick)
time <- list()
meanlcl <- list()
counter <- 0
for (i in 1:ek[3]){
r <- subset(NARR_brick,i) %>% # running each time slice (works best)
projectRaster(crs = crs(var.sictif))  %>% #will not work with EPSG#
crop(y = ROI) #%>% # crop to the DBO3 extent
#resample(y = chldbo3varlay1) # Resamples to the chlorophyll pixel extents
counter <-  counter + 1 # keep track of which layer we are on in the console
print(paste0(counter, " out of ", ek[3]))
time <- append(time, names(r)) # add raster name to a list
k <- cellStats(x = r, stat = "mean") # calculate a mean over ROI
meanlcl <- append(meanlcl, k) # add averaged variable to a list
}
# make a dataframe with the raster name (time) and averaged variable lists
nam <- paste0(deparse(substitute(NARR_brick)), ".csv") # for file naming
df <- do.call(rbind, Map(data.frame, Time=time, Variable=meanlcl))
names(df)[names(df) == 'Variable'] <- substr(nam,5,8) #rename var column
df$Year.month.day <- paste0(substr(files, 53, 56), "_",
substr(files, 57, 59), "_", pat)
# export to a csv
write.csv(df, file = paste("/Users/claregaffey/Documents/RClass/", nam),
row.names = FALSE)#here::here(paste("external/data/", nam)))
return(head(df)) # display some rows of our dataframe
}
# Run for all chlorophyll data
CHL_dataprep(var.chla, dbo3)
#^^^^^^^^^^^^^^^^^^^^^^^
# Bring in all of the chlorophyll time series data
mystack <- stack()
files <- list.files(path="/Volumes/My Passport/RProject2021/MODIS_chl",
pattern="*.nc", full.names=TRUE, recursive=FALSE)
for (x in files) {
chlbrick <- brick(x, varname="chlor_a") # create a raster brick
mystack <- stack(mystack, chlbrick) #stack all of the bricks
}
var.chla <- brick(mystack) # create a brick from the stack
# Modified function to accomodate the chlorophyll file dates
CHL_dataprep <- function(NARR_brick, ROI) {
ek <- dim(NARR_brick)
time <- list()
meanlcl <- list()
counter <- 0
for (i in 1:ek[3]){
r <- subset(NARR_brick,i) %>% # running each time slice (works best)
projectRaster(crs = crs(var.sictif))  %>% #will not work with EPSG#
crop(y = ROI) #%>% # crop to the DBO3 extent
#resample(y = chldbo3varlay1) # Resamples to the chlorophyll pixel extents
counter <-  counter + 1 # keep track of which layer we are on in the console
print(paste0(counter, " out of ", ek[3]))
time <- append(time, names(r)) # add raster name to a list
k <- cellStats(x = r, stat = "mean") # calculate a mean over ROI
meanlcl <- append(meanlcl, k) # add averaged variable to a list
}
# make a dataframe with the raster name (time) and averaged variable lists
nam <- paste0(deparse(substitute(NARR_brick)), ".csv") # for file naming
df <- do.call(rbind, Map(data.frame, Time=time, Variable=meanlcl))
names(df)[names(df) == 'Variable'] <- substr(nam,5,8) #rename var column
df$Year.month.day <- paste0(substr(files, 53, 56), "_",
substr(files, 57, 59), "_", pat)
# export to a csv
write.csv(df, file = paste("/Users/claregaffey/Documents/RClass/", nam),
row.names = FALSE)#here::here(paste("external/data/", nam)))
return(head(df)) # display some rows of our dataframe
}
# Run for all chlorophyll data
CHL_dataprep(var.chla, dbo3)
