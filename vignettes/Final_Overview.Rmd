---
title: "Final Overview"
output: rmarkdown::html_vignette
authors: Aandishah Tehzeeb Samara, Serina Khalifa, Clare Gaffey
date: "2021-11-10"
vignette: >
  %\VignetteIndexEntry{overview}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Introduction (or Overview) --Serina
A more detailed and informative overview of the project's purpose and objectives than what your provided in assignment 6. Approximate maximum length of 400 (individual) to 800 (team, assuming a single package vignette) words.  

Gradient Boosting is a machine learning technique used for regression and classification.
Similar to random forest, it uses an ensemble of decision trees to learn how independent
variables predict the dependent variable. Unlike random forest, trees are built upon
through boosting, so trees are iteratively improved instead of averaged out by
many individual trees. This package extracts information from several raster datasets
and builds a gradient boosting model to determine the most important variables
that influence low cloud cover concentration over the Distributed Biological Observatory (DBO) region in the Chukchi Sea.

#### Methods (or Approach) -all
For a more analytical project, provide a complete description of the methods you used to undertake the analysis, complete with informative illustrations and summaries of the input datasets. Approximate maximum length of 400 (individual) to 800 words. 
For a project that is more tools-oriented, use this section to describe the purpose and rationale of the functions/capabilities you are developing, what other packages/software/tools it is building on, and provide informative illustrations and summaries of the dataset(s) you are using to demonstrate your functions. Same approximate length. 

## Data:
- NARR Reanalysis: (1979-2020)
  - Cloud cover at three altitude levels (low, medium, and high)
  - Wind speed at 10 m
  - Air temperature
  - Evaporation
  - Geopotential height
  - Relative humidity
- NASA MODIS-Aqua chlorophyll concentration (2002-2020)
- DoD SSMI/S sea ice concentration (1979-2020)

## Methods: 
#### Gradient Boosting Regression to find the drivers of low cloud cover concentration in the Chukchi Sea. 
Monthly composites were downloaded (external/data/Download_Datasets.R) for low cloud cover and all of the independent variables for the full extent of their records. Variables were converted to raster bricks, projected, cropped to the region of interest, and monthly averages were saved to a dataframe and exported to a csv (R/Prepare_Rasters_for_Model.R) . Two functions were created in this process; one tailored to the netcdf formats that the NARR Reanalysis time series data were provided in and the second for the chlorophyll data, which consisted of individual netcdf files with each file containing one month of data. Resampling to the chlorophyll pixel extent was also included in the NARR_dataprep function to be used in pixel-by-pixel regression modeling. For this demo however, this feature is commented out and the monthly variables were instead averaged over the region of interest (DBO3 in the Chukchi Sea). The output csvs were formatted to have matching years and dates and then merged and saved into a master csv (R/XGB_Variable_Merge.R). The datasets are large and take hours to process, so breaking up data processing into steps and saving outputs as csvs was important to be able to revisit the project without risk of loss of information from the global environment. For the gradient boosting model (Gradient_Boosting_Model.R), the data were split into 80% training and 20% testing datasets using random uniformly distributed selection of indexes of the dataframe. The gradient boosted model was first run using default settings. From the initial run, the optimal number of trees that produced the minimum Root Mean Square Error (RMSE) was used as an input to tune a second XGBoost model. The accuracy assessment included RMSE and Mean Absolute Error (MAE) and predicted versus actual low cloud cover was plotted for the test dataset. 


## Results (Worked Examples) --all
For analytical projects, describe and illustrate your results, using standard scientific reporting conventions: 1) plan on having 2-4 figures; 2) a similar number of statistical summaries; 3) describe the results in your figures and tables in (up to) 400-800 words (range refers to individual versus team efforts). 
For tools-oriented projects, provide and illustrate 2-4 worked examples of how to use the functions in your package, with descriptive accompanying text that will help users to understand what the functions do and how to apply them (up to 400-800 words; range refers to individual versus team efforts). 


#### Gradient Bossting Model
The initial run of the gradient boosted model provided 23 as the optimum number of trees to reduce RMSE. The next model run on the training data tuned with 23 trees. The model predictions compared to the test datasets produced a MAE of -0.6 and a RMSE of 12.
```{r, echo=FALSE, out.height = 400, out.width = 500}
knitr::include_graphics(here("images/XGB_LCC_vs_predicted_testset.png"))
```

The gain, cover, and frequency was determined for each variable. For brevity, the order of variable importance is illustrated in the bar plot below. Evaporation was the major influencing variable on low cloud cover concentration. The remaining variables were wind speed at 10 m, geopotential height, air temperature, relative humidity, sea ice, and ocean chlorophyll in order of most important to least important.


```{r, echo=FALSE, out.height = 400, out.width = 500}
knitr::include_graphics(here("images/XGB_VariableImportance.png"))
```


#### Discussion --all
For analytical projects, briefly provide your interpretation of the results, any uncertainties/difficulties encountered, and any next steps to be taken. Up to 400-800 words (range refers to individual versus team efforts).
For tools-oriented projects, describe any limitations of the package, improvements that can be made, and any plans to undertake these. Up to 400-800 words (range refers to individual versus team efforts).  

The gradient boosted model performed poorly for the test dataset, as indicated by the RMSE and MAE. This suggests that we did not include essential variables for determining the drivers for low cloud cover. More steps can be made to fine tune the model that would increase the model's accuracy. Future work would include incorporating more datasets as independent variables, and taking advantage of more advanced features of XGBoost to tune the model.
